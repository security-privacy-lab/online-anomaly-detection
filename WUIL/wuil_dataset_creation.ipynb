{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create 3 types of the dataset: \n",
    "1. Random injection - We are going to get random indexes inside of the benign dataset and going to insert the random malicious data into there regarding the session ID (which is the timestamp in integer)\n",
    "2. 5 minute injection - We are going to find the 5 minute gap that exists in the benign dataset and going to inject the **whole** attacking dataset into the benign therefore it would have 5 minute(since one attacking dataset is 5 minute) malicious activities\n",
    "3. Organized dataset - This dataset we are going to combine the malicious dataset and benign dataset(by pandas concat method) and going to sort the dataset by the session ID therefore they are going to be organized. \n",
    "----\n",
    "\n",
    "\n",
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Session_ID</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Path</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40675566</td>\n",
       "      <td>4</td>\n",
       "      <td>0\\1\\2\\3\\4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>40675566</td>\n",
       "      <td>4</td>\n",
       "      <td>0\\1\\2\\3\\4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>40675566</td>\n",
       "      <td>4</td>\n",
       "      <td>0\\1\\2\\3\\4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>40675566</td>\n",
       "      <td>4</td>\n",
       "      <td>0\\1\\2\\3\\4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>40675566</td>\n",
       "      <td>4</td>\n",
       "      <td>0\\1\\2\\3\\4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21657</th>\n",
       "      <td>14908</td>\n",
       "      <td>40242060</td>\n",
       "      <td>7</td>\n",
       "      <td>0\\1\\2\\3\\180\\677\\695\\697</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21658</th>\n",
       "      <td>14909</td>\n",
       "      <td>40242060</td>\n",
       "      <td>8</td>\n",
       "      <td>0\\1\\2\\3\\180\\677\\695\\697\\698</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21659</th>\n",
       "      <td>14910</td>\n",
       "      <td>40242060</td>\n",
       "      <td>8</td>\n",
       "      <td>0\\1\\2\\3\\180\\677\\695\\697\\698</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21660</th>\n",
       "      <td>14911</td>\n",
       "      <td>40242060</td>\n",
       "      <td>9</td>\n",
       "      <td>0\\1\\2\\3\\180\\677\\695\\697\\698\\699</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21661</th>\n",
       "      <td>14912</td>\n",
       "      <td>40242060</td>\n",
       "      <td>8</td>\n",
       "      <td>0\\1\\2\\3\\180\\677\\695\\697\\698</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21662 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  Session_ID  Depth                             Path  Label\n",
       "0          0    40675566      4                        0\\1\\2\\3\\4      1\n",
       "1          1    40675566      4                        0\\1\\2\\3\\4      1\n",
       "2          2    40675566      4                        0\\1\\2\\3\\4      1\n",
       "3          3    40675566      4                        0\\1\\2\\3\\4      1\n",
       "4          4    40675566      4                        0\\1\\2\\3\\4      1\n",
       "...      ...         ...    ...                              ...    ...\n",
       "21657  14908    40242060      7          0\\1\\2\\3\\180\\677\\695\\697      1\n",
       "21658  14909    40242060      8      0\\1\\2\\3\\180\\677\\695\\697\\698      1\n",
       "21659  14910    40242060      8      0\\1\\2\\3\\180\\677\\695\\697\\698      1\n",
       "21660  14911    40242060      9  0\\1\\2\\3\\180\\677\\695\\697\\698\\699      1\n",
       "21661  14912    40242060      8      0\\1\\2\\3\\180\\677\\695\\697\\698      1\n",
       "\n",
       "[21662 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['ID', 'Date', 'Time', 'Session_ID', 'Depth', 'Path', 'Label'] # Set the names for each columns\n",
    "dataset_benign = pd.read_csv(\"user1_log.txt\", sep='|', header = None, names = columns ) # Read the datasets --> Change this line as needed depending on the name of the dataset \n",
    "dataset_benign.drop(['Date','Time'], axis = 1, inplace = True) # Drop the date and time since we are going to use session_ID column for the time\n",
    "dataset_benign = dataset_benign.sort_values(by = 'Session_ID').reset_index(drop = True)\n",
    "dataset_benign['Label'] = 0\n",
    "\n",
    "## same for these lines, change the names if needed. \n",
    "\n",
    "malicious_logs1 = pd.read_csv('Attack1_log.txt', sep='|', header=None, names=columns)\n",
    "malicious_logs2 = pd.read_csv('Attack2_log.txt', sep='|', header=None, names=columns)\n",
    "malicious_logs3 = pd.read_csv('Attack3_log.txt', sep='|', header=None, names=columns)\n",
    "malicious_logs1['Label'] = 1\n",
    "malicious_logs2['Label'] = 1\n",
    "malicious_logs3['Label'] = 1\n",
    "\n",
    "combined_malicious_logs = pd.concat([malicious_logs1, malicious_logs2, malicious_logs3], ignore_index= True)\n",
    "combined_malicious_logs['Label'] = 1\n",
    "combined_malicious_logs.drop(['Date','Time'], axis = 1, inplace = True)\n",
    "combined_malicious_logs\n",
    "\n",
    "combined_malicious_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sedanspot\n",
    "Start generating the Sedanspot dataset which is in format of **Timestamp, source, Destination, weight and label**\n",
    "\n",
    "Lets work on the first type of the dataset which is having random indexes and inserting malicious dataset into the benign "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injected_dataset = dataset_benign.copy() # We copy the benign dataset therefore we don't have to import the benign dataset everytime when we work on it\n",
    "injected_indices = [] # this is for checking if the dataset really have been randomly inserted\n",
    "f = combined_malicious_logs # Which dataset we are going to insert. We adjust this line for changing which file we are trying to insert\n",
    "for _, malicious_row in f.iterrows(): # We read the malicious dataset here\n",
    "        malicious_data = {\n",
    "                'Session_ID': malicious_row['Session_ID'],\n",
    "                'Depth': malicious_row['Depth'],\n",
    "                'Path': malicious_row['Path'],\n",
    "                'Label': malicious_row['Label']\n",
    "    }\n",
    "        malicious_row_df = pd.DataFrame([malicious_data])\n",
    "        \n",
    "        random_index = random.randint(0, len(injected_dataset)) # To make sure that the malicious are being injected to random index \n",
    "        injected_indices.append(random_index)\n",
    "        injected_dataset = pd.concat([\n",
    "                injected_dataset.iloc[:random_index],  \n",
    "                malicious_row_df,                      \n",
    "                injected_dataset.iloc[random_index:]  \n",
    "        ]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Injected {len(f)} malicious rows into the benign dataset.\")\n",
    "print(f\"Original benign dataset length: {len(dataset_benign)}\")\n",
    "print(f\"Injected dataset length: {len(injected_dataset)}\")\n",
    "print(injected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "prev_path = None\n",
    "\n",
    "for _, row in injected_dataset.iterrows():\n",
    "    current_path = row['Path']\n",
    "    timestamp = row['Session_ID']\n",
    "    label = row['Label']\n",
    "\n",
    "    # Self-edge\n",
    "    if prev_path == current_path:\n",
    "        edges.append({\n",
    "            'src_node' : current_path,\n",
    "            'dst_node': current_path,\n",
    "            'timestamp' : timestamp,\n",
    "            'weight' : 1,\n",
    "            'label' : label\n",
    "            \n",
    "        })\n",
    "    elif prev_path is not None:\n",
    "        edges.append({      \n",
    "            'src_node' : prev_path,\n",
    "            'dst_node': current_path,\n",
    "            'timestamp' : row['Session_ID'],\n",
    "            'weight' : 1, # Keeps the weight as 1 as default \n",
    "            'label' : label\n",
    "        })\n",
    "    prev_path = current_path\n",
    "\n",
    "edges_df = pd.DataFrame(edges)\n",
    "edges_df[['timestamp','src_node', 'dst_node',  'weight', 'label']].to_csv(\n",
    "    'random_combined.csv', sep = ',', header = False, index = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work on the 5 minute gaps as we start by finding the 5 minute gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Session_ID  time_diff\n",
      "862       35414007     4035.0\n",
      "2092      35482614    51278.0\n",
      "3164      35504865     4653.0\n",
      "4049      35568126    49681.0\n",
      "6403      35655762    50812.0\n",
      "...            ...        ...\n",
      "257213    40319583   176128.0\n",
      "260670    40404232    48167.0\n",
      "262195    40422830     3068.0\n",
      "266526    40491750    49672.0\n",
      "271546    40574674    48456.0\n",
      "\n",
      "[80 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "dataset_benign['time_diff'] = dataset_benign['Session_ID'].diff()\n",
    "dataset_benign = dataset_benign.sort_values(by = \"Session_ID\", ascending= True)\n",
    "gaps = dataset_benign[dataset_benign['time_diff'] >= 3000]\n",
    "print(gaps[['Session_ID', 'time_diff']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injected_dataset = dataset_benign.copy()\n",
    "injected_dataset = pd.concat([injected_dataset.iloc[:862], # We insert the malicious_logs1 into the gap \n",
    "                    malicious_logs1,\n",
    "                    injected_dataset.iloc[862:]])\n",
    "print(len(injected_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organized version by session ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injected_dataset = dataset_benign.copy()\n",
    "injected_dataset = pd.concat((injected_dataset, combined_malicious_logs),ignore_index= True)\n",
    "injected_dataset = injected_dataset.sort_values(by=\"Session_ID\", ascending=True)\n",
    "injected_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for converting the formatted data into the graph that self-edges and into csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "prev_path = None\n",
    "edge_weights = {}  \n",
    "for _, row in injected_dataset.iterrows():\n",
    "    current_path = row['Path']\n",
    "    timestamp = row['Session_ID']\n",
    "    label = row['Label']\n",
    "\n",
    "    # Self-edge\n",
    "    if prev_path == current_path:\n",
    "        edge_key = (current_path, current_path)  \n",
    "    elif prev_path is not None:\n",
    "        edge_key = (prev_path, current_path)  \n",
    "    else:\n",
    "        prev_path = current_path\n",
    "        continue  \n",
    "\n",
    "\n",
    "    if edge_key in edge_weights:\n",
    "        edge_weights[edge_key] += 1\n",
    "    else:\n",
    "        edge_weights[edge_key] = 1\n",
    "\n",
    "    edges.append({\n",
    "        'src_node': edge_key[0],\n",
    "        'dst_node': edge_key[1],\n",
    "        'timestamp': timestamp,\n",
    "        'weight': edge_weights[edge_key], # Increment the weight as the same node appears \n",
    "        'label': label\n",
    "    })\n",
    "\n",
    "    prev_path = current_path  \n",
    "\n",
    "\n",
    "edges_df = pd.DataFrame(edges)\n",
    "edges_df\n",
    "edges_df[['timestamp', 'src_node', 'dst_node', 'weight', 'label']].to_csv(\n",
    "    'random_attack3_with.csv', sep = ',', header = False, index = False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomrank "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injected_dataset = dataset_benign.copy() # We copy the benign dataset therefore we don't have to import the benign dataset everytime when we work on it\n",
    "injected_indices = [] # this is for checking if the dataset really have been randomly inserted\n",
    "f = combined_malicious_logs # Which dataset we are going to insert. We adjust this line for changing which file we are trying to insert\n",
    "for _, malicious_row in f.iterrows(): # We read the malicious dataset here\n",
    "        malicious_data = {\n",
    "                'Session_ID': malicious_row['Session_ID'],\n",
    "                'Depth': malicious_row['Depth'],\n",
    "                'Path': malicious_row['Path'],\n",
    "                'Label': malicious_row['Label']\n",
    "    }\n",
    "        malicious_row_df = pd.DataFrame([malicious_data])\n",
    "        \n",
    "        random_index = random.randint(0, len(injected_dataset)) # To make sure that the malicious are being injected to random index \n",
    "        injected_indices.append(random_index)\n",
    "        injected_dataset = pd.concat([\n",
    "                injected_dataset.iloc[:random_index],  \n",
    "                malicious_row_df,                      \n",
    "                injected_dataset.iloc[random_index:]  \n",
    "        ]).reset_index(drop=True)\n",
    "\n",
    "print(f\"Injected {len(f)} malicious rows into the benign dataset.\")\n",
    "print(f\"Original benign dataset length: {len(dataset_benign)}\")\n",
    "print(f\"Injected dataset length: {len(injected_dataset)}\")\n",
    "print(injected_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injected_dataset = dataset_benign.copy()\n",
    "injected_dataset = pd.concat([injected_dataset.iloc[:862],\n",
    "                    combined_malicious_logs,\n",
    "                    injected_dataset.iloc[862:]])\n",
    "print(len(injected_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injected_dataset = dataset_benign.copy()\n",
    "injected_dataset = pd.concat((injected_dataset, combined_malicious_logs))\n",
    "injected_dataset = injected_dataset.sort_values(by = \"Session_ID\", ascending= True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib \n",
    "\n",
    "edges = []\n",
    "prev_path = None\n",
    "\n",
    "for _, row in injected_dataset.iterrows():\n",
    "    current_path = row['Path']\n",
    "    timestamp = row['Session_ID']\n",
    "    label = row['Label']\n",
    "    current_path_hashed = int(hashlib.md5(current_path.encode()).hexdigest(), 16) % (10**8)\n",
    "\n",
    "    # Self-edge\n",
    "    if prev_path == current_path:\n",
    "        edges.append({\n",
    "            'src_node' : current_path_hashed,\n",
    "            'dst_node': current_path_hashed,\n",
    "            'timestamp' : timestamp,\n",
    "            'label' : label\n",
    "            \n",
    "        })\n",
    "    elif prev_path is not None:\n",
    "        prev_path_hashed = int(hashlib.md5(prev_path.encode()).hexdigest(), 16) % (10**8)\n",
    "        edges.append({      \n",
    "            'src_node' : prev_path_hashed,\n",
    "            'dst_node': current_path_hashed,\n",
    "            'timestamp' : row['Session_ID'],\n",
    "            'label' : label\n",
    "        })\n",
    "    prev_path = current_path\n",
    "\n",
    "edges_df = pd.DataFrame(edges)\n",
    "edges_df[['timestamp','src_node', 'dst_node', 'label']].to_csv(\n",
    "    'organized_combined.txt', sep = ' ', header = False, index = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
